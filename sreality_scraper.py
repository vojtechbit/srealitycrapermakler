#!/usr/bin/env python3
"""
Sreality.cz Scraper - LokÃ¡lnÃ­ scraper pro stahovÃ¡nÃ­ nabÃ­dek nemovitostÃ­
Autor: Claude (Anthropic)
"""

import requests
import time
import random
import pandas as pd
from datetime import datetime
from typing import List, Dict, Optional
import json
import sys
from pathlib import Path

# Konfigurace
class Config:
    """Konfigurace scraperu"""
    
    # API endpoints
    BASE_URL = "https://www.sreality.cz"
    API_URL = f"{BASE_URL}/api/cs/v2/estates"
    
    # VÃ½chozÃ­ parametry
    CATEGORY_MAIN = {
        1: "Byty",
        2: "Domy", 
        3: "Pozemky",
        4: "KomerÄnÃ­",
        5: "OstatnÃ­"
    }
    
    CATEGORY_TYPE = {
        1: "Prodej",
        2: "PronÃ¡jem",
        3: "DraÅ¾by"
    }
    
    # Rate limiting
    MIN_DELAY = 2  # minimÃ¡lnÃ­ delay mezi requesty (sekundy)
    MAX_DELAY = 5  # maximÃ¡lnÃ­ delay
    
    # User agents pro rotaci
    USER_AGENTS = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    ]
    
    # Output
    OUTPUT_DIR = Path(__file__).parent / "data"
    MAX_PAGES = 50  # maximum strÃ¡nek k prochÃ¡zenÃ­


class SrealityScraper:
    """HlavnÃ­ tÅ™Ã­da scraperu"""
    
    def __init__(self, verbose: bool = True):
        self.config = Config()
        self.session = requests.Session()
        self.verbose = verbose
        self.results: List[Dict] = []
        
        # VytvoÅ™enÃ­ sloÅ¾ky pro data
        self.config.OUTPUT_DIR.mkdir(exist_ok=True)
    
    def _get_headers(self) -> Dict[str, str]:
        """Generuje nÃ¡hodnÃ© HTTP hlaviÄky"""
        return {
            'User-Agent': random.choice(self.config.USER_AGENTS),
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'cs-CZ,cs;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://www.sreality.cz/',
        }
    
    def _delay(self):
        """NÃ¡hodnÃ¡ pauza mezi requesty"""
        delay = random.uniform(self.config.MIN_DELAY, self.config.MAX_DELAY)
        if self.verbose:
            print(f"  â³ ÄŒekÃ¡m {delay:.2f}s...", end='\r')
        time.sleep(delay)
    
    def _make_request(self, url: str, params: Dict, retries: int = 3) -> Optional[Dict]:
        """Provede HTTP request s retry logikou"""
        for attempt in range(retries):
            try:
                headers = self._get_headers()
                response = self.session.get(url, params=params, headers=headers, timeout=30)
                
                if response.status_code == 200:
                    return response.json()
                elif response.status_code == 429:
                    # Rate limit - poÄkÃ¡me dÃ©le
                    wait_time = (2 ** attempt) * 5
                    if self.verbose:
                        print(f"  âš ï¸  Rate limit! ÄŒekÃ¡m {wait_time}s...")
                    time.sleep(wait_time)
                    continue
                else:
                    if self.verbose:
                        print(f"  âŒ Chyba {response.status_code}")
                    return None
                    
            except Exception as e:
                if self.verbose:
                    print(f"  âŒ VÃ½jimka: {str(e)}")
                if attempt < retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                return None
        
        return None
    
    def scrape_listings(self, 
                       category_main: int = 1,  # 1=Byty
                       category_type: int = 1,  # 1=Prodej
                       locality_region_id: Optional[int] = None,  # napÅ™. 10=Praha
                       max_pages: Optional[int] = None) -> List[Dict]:
        """
        StÃ¡hne nabÃ­dky z Sreality
        
        Args:
            category_main: Typ nemovitosti (1=Byty, 2=Domy, 3=Pozemky, 4=KomerÄnÃ­, 5=OstatnÃ­)
            category_type: Typ inzerÃ¡tu (1=Prodej, 2=PronÃ¡jem, 3=DraÅ¾by)
            locality_region_id: ID kraje (volitelnÃ©, napÅ™. 10=Praha, 14=MoravskoslezskÃ½)
            max_pages: MaximÃ¡lnÃ­ poÄet strÃ¡nek (None = vÅ¡e)
        """
        
        if max_pages is None:
            max_pages = self.config.MAX_PAGES
        
        category_main_name = self.config.CATEGORY_MAIN.get(category_main, "NeznÃ¡mÃ©")
        category_type_name = self.config.CATEGORY_TYPE.get(category_type, "NeznÃ¡mÃ©")
        
        print(f"\nğŸ  Sreality Scraper")
        print(f"=" * 60)
        print(f"ğŸ“‹ Typ: {category_main_name} - {category_type_name}")
        if locality_region_id:
            print(f"ğŸ“ Region ID: {locality_region_id}")
        print(f"ğŸ“„ Max. strÃ¡nek: {max_pages}")
        print(f"=" * 60 + "\n")
        
        page = 1
        total_scraped = 0
        
        while page <= max_pages:
            params = {
                'category_main_cb': category_main,
                'category_type_cb': category_type,
                'page': page,
                'per_page': 60,  # zvÃ½Å¡enÃ½ poÄet pro rychlejÅ¡Ã­ scraping
            }
            
            if locality_region_id:
                params['locality_region_id'] = locality_region_id
            
            if self.verbose:
                print(f"ğŸ“„ StrÃ¡nka {page}/{max_pages}...", end=' ')
            
            data = self._make_request(self.config.API_URL, params)
            
            if not data:
                print(f"  âŒ Chyba pÅ™i stahovÃ¡nÃ­ strÃ¡nky {page}")
                break
            
            # ZpracovÃ¡nÃ­ vÃ½sledkÅ¯
            estates = data.get('_embedded', {}).get('estates', [])
            
            if not estates:
                print(f"  â„¹ï¸  Å½Ã¡dnÃ© dalÅ¡Ã­ nabÃ­dky")
                break
            
            for estate in estates:
                parsed = self._parse_estate(estate)
                if parsed:
                    self.results.append(parsed)
            
            total_scraped += len(estates)
            
            if self.verbose:
                print(f"âœ… StaÅ¾eno {len(estates)} nabÃ­dek (celkem: {total_scraped})")
            
            # Kontrola, jestli existuje dalÅ¡Ã­ strÃ¡nka
            result_size = data.get('result_size', 0)
            if page * 60 >= result_size:
                print(f"\nâœ… DosaÅ¾en konec vÃ½sledkÅ¯ ({result_size} celkem)")
                break
            
            page += 1
            self._delay()
        
        print(f"\nâœ¨ Scraping dokonÄen! Celkem nabÃ­dek: {len(self.results)}")
        return self.results
    
    def _parse_estate(self, estate: Dict) -> Optional[Dict]:
        """Parsuje data jednÃ© nabÃ­dky"""
        try:
            # ZÃ¡kladnÃ­ informace
            parsed = {
                'hash_id': estate.get('hash_id'),
                'name': estate.get('name'),
                'locality': estate.get('locality'),
                'price': estate.get('price'),
                'price_czk': estate.get('price_czk', {}).get('value_raw'),
                'price_note': estate.get('price_note'),
            }
            
            # URL
            seo = estate.get('seo', {})
            parsed['url'] = f"{self.config.BASE_URL}{seo.get('href', '')}" if seo.get('href') else None
            
            # Detaily
            items = estate.get('_embedded', {}).get('favourite', {}).get('_embedded', {}).get('items', [])
            for item in items:
                item_name = item.get('name', '').lower()
                item_value = item.get('value')
                
                if 'podlaÅ¾Ã­' in item_name or 'floor' in item_name:
                    parsed['floor'] = item_value
                elif 'plocha' in item_name or 'area' in item_name:
                    parsed['area'] = item_value
                elif 'stavba' in item_name or 'building' in item_name:
                    parsed['building_type'] = item_value
                elif 'vlastnictvÃ­' in item_name or 'ownership' in item_name:
                    parsed['ownership'] = item_value
            
            # ZÃ­skÃ¡nÃ­ plnÃ© adresy z locality
            if estate.get('locality'):
                parsed['full_address'] = estate.get('locality')
            
            # Datum pÅ™idÃ¡nÃ­
            parsed['scraped_at'] = datetime.now().isoformat()
            
            return parsed
            
        except Exception as e:
            if self.verbose:
                print(f"  âš ï¸  Chyba pÅ™i parsovÃ¡nÃ­: {str(e)}")
            return None
    
    def save_to_excel(self, filename: Optional[str] = None) -> str:
        """UloÅ¾Ã­ vÃ½sledky do Excel souboru"""
        if not self.results:
            print("âš ï¸  Å½Ã¡dnÃ¡ data k uloÅ¾enÃ­!")
            return ""
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"sreality_data_{timestamp}.xlsx"
        
        filepath = self.config.OUTPUT_DIR / filename
        
        # VytvoÅ™enÃ­ DataFrame
        df = pd.DataFrame(self.results)
        
        # UloÅ¾enÃ­ do Excelu
        df.to_excel(filepath, index=False, engine='openpyxl')
        
        print(f"\nğŸ’¾ Data uloÅ¾ena do: {filepath}")
        print(f"ğŸ“Š PoÄet zÃ¡znamÅ¯: {len(self.results)}")
        print(f"ğŸ“‹ Sloupce: {', '.join(df.columns.tolist())}")
        
        return str(filepath)
    
    def save_to_csv(self, filename: Optional[str] = None) -> str:
        """UloÅ¾Ã­ vÃ½sledky do CSV souboru (alternativa k Excelu)"""
        if not self.results:
            print("âš ï¸  Å½Ã¡dnÃ¡ data k uloÅ¾enÃ­!")
            return ""
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"sreality_data_{timestamp}.csv"
        
        filepath = self.config.OUTPUT_DIR / filename
        
        df = pd.DataFrame(self.results)
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        
        print(f"\nğŸ’¾ Data uloÅ¾ena do: {filepath}")
        return str(filepath)


def main():
    """HlavnÃ­ funkce - spouÅ¡tÄ›nÃ­ z pÅ™Ã­kazovÃ© Å™Ã¡dky"""
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           SREALITY.CZ SCRAPER                             â•‘
â•‘           LokÃ¡lnÃ­ scraper pro stahovÃ¡nÃ­ nabÃ­dek           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # VytvoÅ™enÃ­ scraperu
    scraper = SrealityScraper(verbose=True)
    
    # PÅ™Ã­klad pouÅ¾itÃ­ - mÅ¯Å¾eÅ¡ zmÄ›nit parametry
    print("\nğŸ“ Zadej parametry vyhledÃ¡vÃ¡nÃ­:")
    print("\nTyp nemovitosti:")
    print("  1 = Byty")
    print("  2 = Domy")
    print("  3 = Pozemky")
    print("  4 = KomerÄnÃ­")
    print("  5 = OstatnÃ­")
    
    try:
        category_main = int(input("\nTyp nemovitosti (1-5) [1]: ") or "1")
    except ValueError:
        category_main = 1
    
    print("\nTyp inzerÃ¡tu:")
    print("  1 = Prodej")
    print("  2 = PronÃ¡jem")
    print("  3 = DraÅ¾by")
    
    try:
        category_type = int(input("\nTyp inzerÃ¡tu (1-3) [1]: ") or "1")
    except ValueError:
        category_type = 1
    
    print("\nKraj (nech prÃ¡zdnÃ© pro celou ÄŒR):")
    print("  10 = Praha")
    print("  11 = StÅ™edoÄeskÃ½")
    print("  12 = JihoÄeskÃ½")
    print("  13 = PlzeÅˆskÃ½")
    print("  14 = MoravskoslezskÃ½")
    print("  (a dalÅ¡Ã­...)")
    
    locality = input("\nID kraje [prÃ¡zdnÃ©]: ").strip()
    locality_id = int(locality) if locality else None
    
    try:
        max_pages = int(input("\nMax. poÄet strÃ¡nek [10]: ") or "10")
    except ValueError:
        max_pages = 10
    
    # SpuÅ¡tÄ›nÃ­ scrapingu
    scraper.scrape_listings(
        category_main=category_main,
        category_type=category_type,
        locality_region_id=locality_id,
        max_pages=max_pages
    )
    
    # UloÅ¾enÃ­ do Excelu
    if scraper.results:
        scraper.save_to_excel()
        
        # NabÃ­dka uloÅ¾enÃ­ i do CSV
        save_csv = input("\nâ“ UloÅ¾it i do CSV? (y/n) [n]: ").lower()
        if save_csv == 'y':
            scraper.save_to_csv()
    
    print("\nâœ¨ Hotovo! MÅ¯Å¾eÅ¡ zavÅ™Ã­t okno.\n")


if __name__ == "__main__":
    main()
