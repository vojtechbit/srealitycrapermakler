#!/usr/bin/env python3
"""
üöÄ SUPER RYCHL√ù scraper makl√©≈ô≈Ø s vyu≈æit√≠m company API

Logika:
1. Projde inzer√°ty, agreguje podle company_id (rychl√©)
2. Pro ka≈ædou company st√°hne seznam makl√©≈ô≈Ø z API (rychl√©!)
3. Vytvo≈ô√≠ hierarchick√Ω Excel: Company ‚Üí Makl√©≈ôi

Rychlost: 4√ó rychlej≈°√≠ ne≈æ p≈ôedchoz√≠ verze!
"""

import argparse
import sys
import re
import unicodedata
from datetime import datetime
from pathlib import Path
from collections import defaultdict

import pandas as pd
from openpyxl import load_workbook
from openpyxl.styles import Font, PatternFill, Alignment
from openpyxl.utils import get_column_letter

from scrapers.sreality import SrealityScraper


def slugify_company_name(name):
    """P≈ôevede n√°zev company na URL-friendly slug."""
    if not name or not isinstance(name, str):
        return "company"
    normalized = unicodedata.normalize("NFKD", name)
    ascii_value = normalized.encode("ascii", "ignore").decode("ascii")
    ascii_value = ascii_value.lower()
    ascii_value = re.sub(r"[^a-z0-9]+", "-", ascii_value)
    ascii_value = re.sub(r"-+", "-", ascii_value)
    return ascii_value.strip("-") or "company"


def scrape_agents_fast(
    scraper,
    category_main,
    category_type,
    locality_region_id,
    max_pages,
    full_scan,
):
    """Super rychl√Ω scraping pomoc√≠ company API."""

    print(f"üîç F√ÅZE 1: Agregace podle company...")

    if full_scan:
        max_pages = None

    limit = max_pages if max_pages is not None else None

    # Agregace podle company_id
    companies = defaultdict(lambda: {
        "company_id": None,
        "company_name": None,
        "total_estates": 0,
        "localities": set(),  # R≈Øzn√© lokality
        "category_breakdown": defaultdict(int),
    })

    page = 1
    total_listings = 0

    # F√ÅZE 1: Projdi inzer√°ty a agreguj podle company
    while True:
        if limit is not None and page > limit:
            break

        params = {
            "category_main_cb": category_main,
            "category_type_cb": category_type,
            "page": page,
            "per_page": 60,
        }

        if locality_region_id is not None:
            params["locality_region_id"] = locality_region_id

        payload = scraper._request(scraper._config.api_url, params=params)
        if not payload:
            print(f"‚ö†Ô∏è  Chyba p≈ôi stahov√°n√≠ str√°nky {page}")
            break

        estates = payload.get("_embedded", {}).get("estates", [])
        if not estates:
            break

        print(f"   Str√°nka {page}: {len(estates)} inzer√°t≈Ø")

        for estate in estates:
            total_listings += 1

            embedded = estate.get("_embedded", {})
            company = embedded.get("company", {})

            if not company:
                continue

            company_id = company.get("id")
            if not company_id:
                continue

            company_id = str(company_id)
            comp = companies[company_id]

            if comp["company_id"] is None:
                comp["company_id"] = company_id
                comp["company_name"] = company.get("name")

            comp["total_estates"] += 1

            # Lokalita
            locality = estate.get("locality", "")
            if locality:
                comp["localities"].add(locality)

            # Kategorie
            seo = estate.get("seo", {}) if isinstance(estate.get("seo"), dict) else {}
            cat_main = seo.get("category_main_cb") or category_main
            cat_type = seo.get("category_type_cb") or category_type
            key = (cat_main, cat_type)
            comp["category_breakdown"][key] += 1

        result_size = payload.get("result_size", 0)
        if (page * 60) >= result_size:
            break

        page += 1
        scraper._delay()

    print(f"\n‚úÖ Zpracov√°no {total_listings} inzer√°t≈Ø")
    print(f"‚úÖ Nalezeno {len(companies)} realitn√≠ch kancel√°≈ô√≠")

    # F√ÅZE 2: Pro ka≈ædou company st√°hni seznam makl√©≈ô≈Ø
    print(f"\nüîç F√ÅZE 2: Stahuji seznam makl√©≈ô≈Ø z company API...")

    all_records = []

    for idx, (company_id, comp) in enumerate(companies.items(), 1):
        company_url = f"{scraper._config.base_url}/api/cs/v2/companies/{company_id}"
        company_data = scraper._request(company_url)

        if not company_data:
            print(f"   ‚ö†Ô∏è  Chyba p≈ôi stahov√°n√≠ company {company_id}")
            continue

        # Z√≠skej seznam makl√©≈ô≈Ø
        embedded = company_data.get("_embedded", {})
        sellers_data = embedded.get("sellers", {})

        if isinstance(sellers_data, dict):
            sellers_list = sellers_data.get("sellers", [])
        else:
            sellers_list = []

        if not sellers_list:
            print(f"   ‚ö†Ô∏è  Company {comp['company_name']}: ≈æ√°dn√≠ makl√©≈ôi")
            continue

        print(f"   {idx}/{len(companies)}: {comp['company_name']} - {len(sellers_list)} makl√©≈ô≈Ø")

        # Lokalita - vezmi nejƒçastƒõj≈°√≠
        localities_list = list(comp["localities"])
        if localities_list:
            # Vezmi prvn√≠ lokalitu a parse kraj/mƒõsto
            locality = localities_list[0]
            parts = [p.strip() for p in locality.split(",")]
            mesto = parts[0] if parts else ""
            kraj = parts[-1] if len(parts) > 1 else ""
        else:
            mesto = ""
            kraj = ""

        # Vytvo≈ô rozlo≈æen√≠ inzer√°t≈Ø
        category_names = {1: "Byty", 2: "Domy", 3: "Pozemky", 4: "Komerƒçn√≠", 5: "Ostatn√≠"}
        type_names = {1: "Prodej", 2: "Pron√°jem", 3: "Dra≈æby"}
        breakdown_items = []
        for (cat, typ), count in sorted(comp["category_breakdown"].items(), key=lambda x: -x[1]):
            cat_name = category_names.get(cat, f"Kategorie {cat}")
            typ_name = type_names.get(typ, f"Typ {typ}")
            breakdown_items.append(f"{cat_name}/{typ_name}: {count}")
        rozlozeni = ", ".join(breakdown_items) if breakdown_items else ""

        # Company ≈ô√°dek (hlaviƒçka)
        company_slug = slugify_company_name(comp["company_name"])

        all_records.append({
            "typ_radku": "COMPANY",  # Speci√°ln√≠ typ pro form√°tov√°n√≠
            "zdroj": "Sreality.cz",
            "realitni_kancelar": comp["company_name"],
            "jmeno_maklere": "",
            "telefon": "",
            "email": "",
            "kraj": kraj,
            "mesto": mesto,
            "profil_url": "",
            "pocet_inzeratu": comp["total_estates"],
            "rozlozeni_inzeratu": rozlozeni,
        })

        # Makl√©≈ôi pod company
        for seller in sellers_list:
            seller_id = seller.get("id")
            seller_name = seller.get("name", "")

            # Telefon - vezmi prvn√≠
            phones = seller.get("phones", [])
            phone = ""
            if phones and isinstance(phones, list):
                first_phone = phones[0]
                if isinstance(first_phone, dict):
                    phone = first_phone.get("number", "")

            email = seller.get("email", "")

            # URL profilu
            profile_url = f"https://www.sreality.cz/adresar/{company_slug}/{company_id}/makleri/{seller_id}"

            all_records.append({
                "typ_radku": "AGENT",  # Makl√©≈ô
                "zdroj": "",
                "realitni_kancelar": "",  # Pr√°zdn√©, je pod hlaviƒçkou
                "jmeno_maklere": seller_name,
                "telefon": phone,
                "email": email,
                "kraj": "",
                "mesto": "",
                "profil_url": profile_url,
                "pocet_inzeratu": "",
                "rozlozeni_inzeratu": "",
            })

        scraper._delay()

    print(f"\n‚úÖ Stahuji dokonƒçeno")

    return all_records


def save_to_excel_hierarchical(records, output_path):
    """Ulo≈æ√≠ do Excelu s hierarchick√Ωm form√°tov√°n√≠m."""
    if not records:
        print("‚ö†Ô∏è  ≈Ω√°dn√© z√°znamy")
        return

    Path(output_path).parent.mkdir(parents=True, exist_ok=True)

    # Odstra≈à typ_radku pro export
    export_records = []
    for rec in records:
        export_rec = {k: v for k, v in rec.items() if k != "typ_radku"}
        export_records.append(export_rec)

    df = pd.DataFrame(export_records)
    df.to_excel(output_path, index=False, engine="openpyxl")

    # Form√°tov√°n√≠
    wb = load_workbook(output_path)
    ws = wb.active

    # Barvy
    company_fill = PatternFill(start_color="E8F4F8", end_color="E8F4F8", fill_type="solid")  # Svƒõtle modr√°
    company_font = Font(bold=True, size=12)

    # Najdi sloupec profil_url
    headers = [cell.value for cell in ws[1]]
    profil_col = None
    for idx, header in enumerate(headers, 1):
        if header == "profil_url":
            profil_col = idx
            break

    # Form√°tuj ≈ô√°dky
    for row_idx in range(2, ws.max_row + 1):
        typ_radku = records[row_idx - 2].get("typ_radku")

        if typ_radku == "COMPANY":
            # Company ≈ô√°dek - zv√Ωrazni
            for col_idx in range(1, ws.max_column + 1):
                cell = ws.cell(row=row_idx, column=col_idx)
                cell.fill = company_fill
                cell.font = company_font
        elif typ_radku == "AGENT":
            # Makl√©≈ô - odsaƒè a p≈ôidej hyperlink
            jmeno_cell = ws.cell(row=row_idx, column=headers.index("jmeno_maklere") + 1 if "jmeno_maklere" in headers else 4)
            jmeno_cell.value = f"  ‚Üí {jmeno_cell.value}"  # Odsazen√≠

            # Hyperlink
            if profil_col:
                cell = ws.cell(row=row_idx, column=profil_col)
                url = cell.value
                if url and isinstance(url, str) and url.startswith("http"):
                    cell.hyperlink = url
                    cell.value = "Profil makl√©≈ôe"
                    cell.font = Font(color="0000FF", underline="single")

    # ≈†√≠≈ôka sloupc≈Ø
    for column in ws.columns:
        max_length = 0
        column_letter = get_column_letter(column[0].column)
        for cell in column:
            try:
                if cell.value:
                    max_length = max(max_length, len(str(cell.value)))
            except:
                pass
        ws.column_dimensions[column_letter].width = min(max_length + 2, 60)

    wb.save(output_path)

    # Spoƒç√≠tej statistiky
    companies_count = sum(1 for r in records if r.get("typ_radku") == "COMPANY")
    agents_count = sum(1 for r in records if r.get("typ_radku") == "AGENT")
    total_estates = sum(r.get("pocet_inzeratu", 0) for r in records if isinstance(r.get("pocet_inzeratu"), int))

    print(f"\n‚úÖ Ulo≈æeno do: {output_path}")
    print(f"üìä Realitn√≠ch kancel√°≈ô√≠: {companies_count}")
    print(f"üë§ Celkem makl√©≈ô≈Ø: {agents_count}")
    print(f"üè† Celkem inzer√°t≈Ø: {total_estates}")


def main():
    parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter)

    parser.add_argument("--category-main", type=int, default=1, help="1=Byty, 2=Domy, ...")
    parser.add_argument("--category-type", type=int, default=1, help="1=Prodej, 2=Pron√°jem, ...")
    parser.add_argument("--locality", type=int, help="10=Praha, 11=St≈ôedoƒçesk√Ω, ...")
    parser.add_argument("--max-pages", type=int, default=5, help="Max str√°nek [5]")
    parser.add_argument("--full-scan", action="store_true", help="V≈°echny str√°nky")
    parser.add_argument("-o", "--output", help="V√Ωstupn√≠ soubor")

    args = parser.parse_args()

    print("="*80)
    print("üöÄ SUPER RYCHL√ù SCRAPER MAKL√â≈ò≈Æ (s company API)")
    print("="*80)
    print()

    category_names = {1: "Byty", 2: "Domy", 3: "Pozemky", 4: "Komerƒçn√≠", 5: "Ostatn√≠"}
    type_names = {1: "Prodej", 2: "Pron√°jem", 3: "Dra≈æby"}
    region_names = {
        10: "Praha", 11: "St≈ôedoƒçesk√Ω", 12: "Jihoƒçesk√Ω", 13: "Plze≈àsk√Ω",
        14: "Karlovarsk√Ω", 15: "√östeck√Ω", 16: "Libereck√Ω", 17: "Kr√°lov√©hradeck√Ω",
        18: "Pardubick√Ω", 19: "Vysoƒçina", 20: "Jihomoravsk√Ω", 21: "Olomouck√Ω",
        22: "Zl√≠nsk√Ω", 23: "Moravskoslezsk√Ω"
    }

    print("üìã Parametry:")
    print(f"   ‚Ä¢ Typ: {category_names.get(args.category_main, 'Nezn√°m√Ω')}")
    print(f"   ‚Ä¢ Inzer√°t: {type_names.get(args.category_type, 'Nezn√°m√Ω')}")
    print(f"   ‚Ä¢ Kraj: {region_names.get(args.locality, 'Cel√° ƒåR')}")
    print(f"   ‚Ä¢ Str√°nek: {'V≈†ECHNY' if args.full_scan else args.max_pages}")
    print()

    try:
        scraper = SrealityScraper()

        records = scrape_agents_fast(
            scraper,
            args.category_main,
            args.category_type,
            args.locality,
            args.max_pages,
            args.full_scan,
        )

        if records:
            output = args.output or f"data/makleri_fast_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
            save_to_excel_hierarchical(records, output)
        else:
            print("‚ö†Ô∏è  ≈Ω√°dn√° data")

    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  P≈ôeru≈°eno")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Chyba: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

    print("\n" + "="*80)
    print("‚úÖ Hotovo!")
    print("="*80)


if __name__ == "__main__":
    main()
