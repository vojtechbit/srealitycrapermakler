#!/usr/bin/env python3
"""
üöÄ SUPER RYCHL√ù scraper makl√©≈ô≈Ø s vyu≈æit√≠m company API

Logika:
1. Projde inzer√°ty, agreguje podle company_id (rychl√©)
2. Pro ka≈ædou company st√°hne seznam makl√©≈ô≈Ø z API (rychl√©!)
3. Vytvo≈ô√≠ hierarchick√Ω Excel: Company ‚Üí Makl√©≈ôi

Rychlost: 4√ó rychlej≈°√≠ ne≈æ p≈ôedchoz√≠ verze!
"""

import argparse
import sys
import re
import unicodedata
from datetime import datetime
from pathlib import Path
from collections import defaultdict

import pandas as pd
from openpyxl import load_workbook
from openpyxl.styles import Font, PatternFill, Alignment
from openpyxl.utils import get_column_letter

from scrapers.sreality import SrealityScraper


def slugify_company_name(name):
    """P≈ôevede n√°zev company na URL-friendly slug."""
    if not name or not isinstance(name, str):
        return "company"
    normalized = unicodedata.normalize("NFKD", name)
    ascii_value = normalized.encode("ascii", "ignore").decode("ascii")
    ascii_value = ascii_value.lower()
    ascii_value = re.sub(r"[^a-z0-9]+", "-", ascii_value)
    ascii_value = re.sub(r"-+", "-", ascii_value)
    return ascii_value.strip("-") or "company"


def scrape_agents_fast_combined(
    scraper,
    combinations,  # List of (category_main, category_type, locality) tuples
    max_pages,
    full_scan,
):
    """
    Super rychl√Ω scraping s deduplikac√≠ companies nap≈ô√≠ƒç kombinacemi.

    F√ÅZE 1: Agreguj companies ze V≈†ECH kombinac√≠
    F√ÅZE 2: Deduplikuj (ka≈æd√° company jen jednou)
    F√ÅZE 3: Volej sellers API jen pro unik√°tn√≠ companies
    """

    print(f"üîç F√ÅZE 1: Agregace companies ze v≈°ech kombinac√≠...")

    if full_scan:
        max_pages = None

    limit = max_pages if max_pages is not None else None

    # Sd√≠len√Ω dictionary pro V≈†ECHNY kombinace!
    all_companies = defaultdict(lambda: {
        "company_id": None,
        "company_name": None,
        "total_estates": 0,
        "localities": set(),
        "category_breakdown": defaultdict(int),
    })

    total_listings_all = 0
    category_names = {1: "Byty", 2: "Domy", 3: "Pozemky", 4: "Komerƒçn√≠", 5: "Ostatn√≠"}
    type_names = {1: "Prodej", 2: "Pron√°jem", 3: "Dra≈æby"}

    # F√ÅZE 1: Projdi V≈†ECHNY kombinace a agreguj do jednoho dictionary
    for combo_idx, (category_main, category_type, locality_region_id) in enumerate(combinations, 1):
        print(f"\n   Kombinace {combo_idx}/{len(combinations)}: {category_names.get(category_main)} / {type_names.get(category_type)}")

        page = 1

        while True:
            if limit is not None and page > limit:
                break

            params = {
                "category_main_cb": category_main,
                "category_type_cb": category_type,
                "page": page,
                "per_page": 60,
            }

            if locality_region_id is not None:
                params["locality_region_id"] = locality_region_id

            payload = scraper._request(scraper._config.api_url, params=params)
            if not payload:
                print(f"      ‚ö†Ô∏è  Chyba p≈ôi stahov√°n√≠ str√°nky {page}")
                break

            estates = payload.get("_embedded", {}).get("estates", [])
            if not estates:
                break

            # Poƒç√≠tadla
            new_companies = 0
            existing_companies = 0

            for estate in estates:
                total_listings_all += 1

                embedded = estate.get("_embedded", {})
                company = embedded.get("company", {})

                if not company:
                    continue

                company_id = company.get("id")
                if not company_id:
                    continue

                company_id = str(company_id)

                # Kontrola, jestli je company nov√° (nap≈ô√≠ƒç V≈†EMI kombinacemi!)
                comp = all_companies[company_id]

                if comp["company_id"] is None:
                    comp["company_id"] = company_id
                    comp["company_name"] = company.get("name")
                    new_companies += 1
                else:
                    existing_companies += 1

                comp["total_estates"] += 1

                # Lokalita
                locality = estate.get("locality", "")
                if locality:
                    comp["localities"].add(locality)

                # Kategorie
                seo = estate.get("seo", {}) if isinstance(estate.get("seo"), dict) else {}
                cat_main = seo.get("category_main_cb") or category_main
                cat_type = seo.get("category_type_cb") or category_type
                key = (cat_main, cat_type)
                comp["category_breakdown"][key] += 1

            # V√Ωpis
            print(f"      Str√°nka {page}: {len(estates)} inzer√°t≈Ø", end="")
            if new_companies > 0 or existing_companies > 0:
                print(f" (Nov√© RK: {new_companies}, Existuj√≠c√≠: {existing_companies})", end="")
            print()

            result_size = payload.get("result_size", 0)
            if (page * 60) >= result_size:
                break

            page += 1
            scraper._delay()

    print(f"\n‚úÖ Zpracov√°no {total_listings_all} inzer√°t≈Ø celkem")
    print(f"‚úÖ Nalezeno {len(all_companies)} UNIK√ÅTN√çCH realitn√≠ch kancel√°≈ô√≠")

    # F√ÅZE 2: Volej sellers API jen pro UNIK√ÅTN√ç companies
    print(f"\nüîç F√ÅZE 2: Stahuji seznam makl√©≈ô≈Ø (jen pro unik√°tn√≠ RK)...")

    all_records = []

    for idx, (company_id, comp) in enumerate(all_companies.items(), 1):
        # St√°hnout V≈†ECHNY makl√©≈ôe (m≈Ø≈æe b√Ωt v√≠ce str√°nek!)
        all_sellers = []
        page = 1

        while True:
            company_url = f"{scraper._config.base_url}/api/cs/v2/companies/{company_id}"
            params = {"page": page} if page > 1 else None
            company_data = scraper._request(company_url, params=params)

            if not company_data:
                print(f"   ‚ö†Ô∏è  Chyba p≈ôi stahov√°n√≠ company {company_id}")
                break

            # Z√≠skej seznam makl√©≈ô≈Ø
            embedded = company_data.get("_embedded", {})
            sellers_data = embedded.get("sellers", {})

            if isinstance(sellers_data, dict):
                result_size = sellers_data.get("result_size", 0)
                per_page = sellers_data.get("per_page", 20)
                sellers_list = sellers_data.get("sellers", [])
            else:
                sellers_list = []
                result_size = 0
                per_page = 20

            if not sellers_list:
                break

            all_sellers.extend(sellers_list)

            # Kontrola, jestli jsou dal≈°√≠ str√°nky
            if (page * per_page) >= result_size:
                break

            page += 1
            scraper._delay()

        if not all_sellers:
            print(f"   ‚ö†Ô∏è  Company {comp['company_name']}: ≈æ√°dn√≠ makl√©≈ôi")
            continue

        # V√Ωpis
        if page > 1:
            print(f"   {idx}/{len(all_companies)}: {comp['company_name']} - {len(all_sellers)} makl√©≈ô≈Ø ({page} str√°nek)")
        else:
            print(f"   {idx}/{len(all_companies)}: {comp['company_name']} - {len(all_sellers)} makl√©≈ô≈Ø")

        # Lokalita
        localities_list = list(comp["localities"])
        if localities_list:
            locality = localities_list[0]
            parts = [p.strip() for p in locality.split(",")]
            mesto = parts[0] if parts else ""
            kraj = parts[-1] if len(parts) > 1 else ""
        else:
            mesto = ""
            kraj = ""

        # Rozlo≈æen√≠
        breakdown_items = []
        for (cat, typ), count in sorted(comp["category_breakdown"].items(), key=lambda x: -x[1]):
            cat_name = category_names.get(cat, f"Kategorie {cat}")
            typ_name = type_names.get(typ, f"Typ {typ}")
            breakdown_items.append(f"{cat_name}/{typ_name}: {count}")
        rozlozeni = ", ".join(breakdown_items) if breakdown_items else ""

        company_slug = slugify_company_name(comp["company_name"])

        # Company ≈ô√°dek
        all_records.append({
            "typ_radku": "COMPANY",
            "zdroj": "Sreality.cz",
            "realitni_kancelar": comp["company_name"],
            "jmeno_maklere": "",
            "telefon": "",
            "email": "",
            "kraj": kraj,
            "mesto": mesto,
            "profil_url": "",
            "pocet_inzeratu": comp["total_estates"],
            "rozlozeni_inzeratu": rozlozeni,
        })

        # Makl√©≈ôi
        for seller in all_sellers:
            seller_id = seller.get("id")
            seller_name = seller.get("name", "")

            phones = seller.get("phones", [])
            phone = ""
            if phones and isinstance(phones, list):
                first_phone = phones[0]
                if isinstance(first_phone, dict):
                    phone = first_phone.get("number", "")

            email = seller.get("email", "")
            profile_url = f"https://www.sreality.cz/adresar/{company_slug}/{company_id}/makleri/{seller_id}"

            all_records.append({
                "typ_radku": "AGENT",
                "zdroj": "",
                "realitni_kancelar": "",
                "jmeno_maklere": seller_name,
                "telefon": phone,
                "email": email,
                "kraj": "",
                "mesto": "",
                "profil_url": profile_url,
                "pocet_inzeratu": "",
                "rozlozeni_inzeratu": "",
            })

        scraper._delay()

    print(f"\n‚úÖ Stahov√°n√≠ dokonƒçeno")

    return all_records


def scrape_agents_fast(
    scraper,
    category_main,
    category_type,
    locality_region_id,
    max_pages,
    full_scan,
):
    """Super rychl√Ω scraping pomoc√≠ company API (single combination)."""

    print(f"üîç F√ÅZE 1: Agregace podle company...")

    if full_scan:
        max_pages = None

    limit = max_pages if max_pages is not None else None

    # Agregace podle company_id
    companies = defaultdict(lambda: {
        "company_id": None,
        "company_name": None,
        "total_estates": 0,
        "localities": set(),  # R≈Øzn√© lokality
        "category_breakdown": defaultdict(int),
    })

    page = 1
    total_listings = 0

    # F√ÅZE 1: Projdi inzer√°ty a agreguj podle company
    while True:
        if limit is not None and page > limit:
            break

        params = {
            "category_main_cb": category_main,
            "category_type_cb": category_type,
            "page": page,
            "per_page": 60,
        }

        if locality_region_id is not None:
            params["locality_region_id"] = locality_region_id

        payload = scraper._request(scraper._config.api_url, params=params)
        if not payload:
            print(f"‚ö†Ô∏è  Chyba p≈ôi stahov√°n√≠ str√°nky {page}")
            break

        estates = payload.get("_embedded", {}).get("estates", [])
        if not estates:
            break

        # Poƒç√≠tadla pro tuto str√°nku
        new_companies = 0
        existing_companies = 0

        for estate in estates:
            total_listings += 1

            embedded = estate.get("_embedded", {})
            company = embedded.get("company", {})

            if not company:
                continue

            company_id = company.get("id")
            if not company_id:
                continue

            company_id = str(company_id)

            # Kontrola, jestli je company nov√°
            is_new = company_id not in companies or companies[company_id]["company_id"] is None

            comp = companies[company_id]

            if comp["company_id"] is None:
                comp["company_id"] = company_id
                comp["company_name"] = company.get("name")
                new_companies += 1
            else:
                existing_companies += 1

            comp["total_estates"] += 1

            # Lokalita
            locality = estate.get("locality", "")
            if locality:
                comp["localities"].add(locality)

            # Kategorie
            seo = estate.get("seo", {}) if isinstance(estate.get("seo"), dict) else {}
            cat_main = seo.get("category_main_cb") or category_main
            cat_type = seo.get("category_type_cb") or category_type
            key = (cat_main, cat_type)
            comp["category_breakdown"][key] += 1

        # V√Ωpis statistik pro tuto str√°nku
        print(f"   Str√°nka {page}: {len(estates)} inzer√°t≈Ø")
        if new_companies > 0 or existing_companies > 0:
            print(f"      ‚Üí Nov√© RK: {new_companies}, Existuj√≠c√≠ RK: {existing_companies}")

        result_size = payload.get("result_size", 0)
        if (page * 60) >= result_size:
            break

        page += 1
        scraper._delay()

    print(f"\n‚úÖ Zpracov√°no {total_listings} inzer√°t≈Ø")
    print(f"‚úÖ Nalezeno {len(companies)} realitn√≠ch kancel√°≈ô√≠")

    # F√ÅZE 2: Pro ka≈ædou company st√°hni seznam makl√©≈ô≈Ø (s paginac√≠!)
    print(f"\nüîç F√ÅZE 2: Stahuji seznam makl√©≈ô≈Ø z company API...")

    all_records = []

    for idx, (company_id, comp) in enumerate(companies.items(), 1):
        # St√°hnout V≈†ECHNY makl√©≈ôe (m≈Ø≈æe b√Ωt v√≠ce str√°nek!)
        all_sellers = []
        page = 1

        while True:
            company_url = f"{scraper._config.base_url}/api/cs/v2/companies/{company_id}"
            params = {"page": page} if page > 1 else None
            company_data = scraper._request(company_url, params=params)

            if not company_data:
                print(f"   ‚ö†Ô∏è  Chyba p≈ôi stahov√°n√≠ company {company_id}")
                break

            # Z√≠skej seznam makl√©≈ô≈Ø
            embedded = company_data.get("_embedded", {})
            sellers_data = embedded.get("sellers", {})

            if isinstance(sellers_data, dict):
                result_size = sellers_data.get("result_size", 0)
                per_page = sellers_data.get("per_page", 20)
                sellers_list = sellers_data.get("sellers", [])
            else:
                sellers_list = []
                result_size = 0
                per_page = 20

            if not sellers_list:
                break

            all_sellers.extend(sellers_list)

            # Kontrola, jestli jsou dal≈°√≠ str√°nky
            if (page * per_page) >= result_size:
                break

            page += 1
            scraper._delay()  # Delay mezi str√°nkami

        if not all_sellers:
            print(f"   ‚ö†Ô∏è  Company {comp['company_name']}: ≈æ√°dn√≠ makl√©≈ôi")
            continue

        # Pokud bylo v√≠ce str√°nek, uka≈æ to
        if page > 1:
            print(f"   {idx}/{len(companies)}: {comp['company_name']} - {len(all_sellers)} makl√©≈ô≈Ø ({page} str√°nek)")
        else:
            print(f"   {idx}/{len(companies)}: {comp['company_name']} - {len(all_sellers)} makl√©≈ô≈Ø")

        # Lokalita - vezmi nejƒçastƒõj≈°√≠
        localities_list = list(comp["localities"])
        if localities_list:
            # Vezmi prvn√≠ lokalitu a parse kraj/mƒõsto
            locality = localities_list[0]
            parts = [p.strip() for p in locality.split(",")]
            mesto = parts[0] if parts else ""
            kraj = parts[-1] if len(parts) > 1 else ""
        else:
            mesto = ""
            kraj = ""

        # Vytvo≈ô rozlo≈æen√≠ inzer√°t≈Ø
        category_names = {1: "Byty", 2: "Domy", 3: "Pozemky", 4: "Komerƒçn√≠", 5: "Ostatn√≠"}
        type_names = {1: "Prodej", 2: "Pron√°jem", 3: "Dra≈æby"}
        breakdown_items = []
        for (cat, typ), count in sorted(comp["category_breakdown"].items(), key=lambda x: -x[1]):
            cat_name = category_names.get(cat, f"Kategorie {cat}")
            typ_name = type_names.get(typ, f"Typ {typ}")
            breakdown_items.append(f"{cat_name}/{typ_name}: {count}")
        rozlozeni = ", ".join(breakdown_items) if breakdown_items else ""

        # Company ≈ô√°dek (hlaviƒçka)
        company_slug = slugify_company_name(comp["company_name"])

        all_records.append({
            "typ_radku": "COMPANY",  # Speci√°ln√≠ typ pro form√°tov√°n√≠
            "zdroj": "Sreality.cz",
            "realitni_kancelar": comp["company_name"],
            "jmeno_maklere": "",
            "telefon": "",
            "email": "",
            "kraj": kraj,
            "mesto": mesto,
            "profil_url": "",
            "pocet_inzeratu": comp["total_estates"],
            "rozlozeni_inzeratu": rozlozeni,
        })

        # Makl√©≈ôi pod company
        for seller in all_sellers:
            seller_id = seller.get("id")
            seller_name = seller.get("name", "")

            # Telefon - vezmi prvn√≠
            phones = seller.get("phones", [])
            phone = ""
            if phones and isinstance(phones, list):
                first_phone = phones[0]
                if isinstance(first_phone, dict):
                    phone = first_phone.get("number", "")

            email = seller.get("email", "")

            # URL profilu
            profile_url = f"https://www.sreality.cz/adresar/{company_slug}/{company_id}/makleri/{seller_id}"

            all_records.append({
                "typ_radku": "AGENT",  # Makl√©≈ô
                "zdroj": "",
                "realitni_kancelar": "",  # Pr√°zdn√©, je pod hlaviƒçkou
                "jmeno_maklere": seller_name,
                "telefon": phone,
                "email": email,
                "kraj": "",
                "mesto": "",
                "profil_url": profile_url,
                "pocet_inzeratu": "",
                "rozlozeni_inzeratu": "",
            })

        scraper._delay()

    print(f"\n‚úÖ Stahuji dokonƒçeno")

    return all_records


def save_to_excel_hierarchical(records, output_path):
    """Ulo≈æ√≠ do Excelu s hierarchick√Ωm form√°tov√°n√≠m."""
    if not records:
        print("‚ö†Ô∏è  ≈Ω√°dn√© z√°znamy")
        return

    Path(output_path).parent.mkdir(parents=True, exist_ok=True)

    # Odstra≈à typ_radku pro export
    export_records = []
    for rec in records:
        export_rec = {k: v for k, v in rec.items() if k != "typ_radku"}
        export_records.append(export_rec)

    df = pd.DataFrame(export_records)
    df.to_excel(output_path, index=False, engine="openpyxl")

    # Form√°tov√°n√≠
    wb = load_workbook(output_path)
    ws = wb.active

    # Barvy
    company_fill = PatternFill(start_color="E8F4F8", end_color="E8F4F8", fill_type="solid")  # Svƒõtle modr√°
    company_font = Font(bold=True, size=12)

    # Najdi sloupec profil_url
    headers = [cell.value for cell in ws[1]]
    profil_col = None
    for idx, header in enumerate(headers, 1):
        if header == "profil_url":
            profil_col = idx
            break

    # Form√°tuj ≈ô√°dky
    for row_idx in range(2, ws.max_row + 1):
        typ_radku = records[row_idx - 2].get("typ_radku")

        if typ_radku == "COMPANY":
            # Company ≈ô√°dek - zv√Ωrazni
            for col_idx in range(1, ws.max_column + 1):
                cell = ws.cell(row=row_idx, column=col_idx)
                cell.fill = company_fill
                cell.font = company_font
        elif typ_radku == "AGENT":
            # Makl√©≈ô - odsaƒè a p≈ôidej hyperlink
            jmeno_cell = ws.cell(row=row_idx, column=headers.index("jmeno_maklere") + 1 if "jmeno_maklere" in headers else 4)
            jmeno_cell.value = f"  ‚Üí {jmeno_cell.value}"  # Odsazen√≠

            # Hyperlink
            if profil_col:
                cell = ws.cell(row=row_idx, column=profil_col)
                url = cell.value
                if url and isinstance(url, str) and url.startswith("http"):
                    cell.hyperlink = url
                    cell.value = "Profil makl√©≈ôe"
                    cell.font = Font(color="0000FF", underline="single")

    # ≈†√≠≈ôka sloupc≈Ø
    for column in ws.columns:
        max_length = 0
        column_letter = get_column_letter(column[0].column)
        for cell in column:
            try:
                if cell.value:
                    max_length = max(max_length, len(str(cell.value)))
            except:
                pass
        ws.column_dimensions[column_letter].width = min(max_length + 2, 60)

    wb.save(output_path)

    # Spoƒç√≠tej statistiky
    companies_count = sum(1 for r in records if r.get("typ_radku") == "COMPANY")
    agents_count = sum(1 for r in records if r.get("typ_radku") == "AGENT")
    total_estates = sum(r.get("pocet_inzeratu", 0) for r in records if isinstance(r.get("pocet_inzeratu"), int))

    print(f"\n‚úÖ Ulo≈æeno do: {output_path}")
    print(f"üìä Realitn√≠ch kancel√°≈ô√≠: {companies_count}")
    print(f"üë§ Celkem makl√©≈ô≈Ø: {agents_count}")
    print(f"üè† Celkem inzer√°t≈Ø: {total_estates}")


def prompt_for_params():
    """Interaktivn√≠ v√Ωbƒõr parametr≈Ø s podporou multiple selection."""
    print("\n" + "="*80)
    print("üìã INTERAKTIVN√ç V√ùBƒöR PARAMETR≈Æ")
    print("="*80)
    print()

    # Kategorie
    print("Typ nemovitosti:")
    print("  1 = Byty")
    print("  2 = Domy")
    print("  3 = Pozemky")
    print("  4 = Komerƒçn√≠")
    print("  5 = Ostatn√≠")
    category_input = input("\nVyber typ nemovitosti (1-5, oddƒõlen√© ƒç√°rkou) [1]: ").strip() or "1"
    categories = [c.strip() for c in category_input.split(",")]
    category_main_list = [int(c) for c in categories if c.isdigit()]

    # Typy inzer√°t≈Ø
    print("\nTyp inzer√°tu:")
    print("  1 = Prodej")
    print("  2 = Pron√°jem")
    print("  3 = Dra≈æby")
    type_input = input("\nVyber typ inzer√°tu (1-3, oddƒõlen√© ƒç√°rkou) [1]: ").strip() or "1"
    types = [t.strip() for t in type_input.split(",")]
    category_type_list = [int(t) for t in types if t.isdigit()]

    # Kraje
    print("\nKraj (voliteln√©):")
    print("  10 = Praha")
    print("  11 = St≈ôedoƒçesk√Ω")
    print("  12 = Jihoƒçesk√Ω")
    print("  13 = Plze≈àsk√Ω")
    print("  14 = Karlovarsk√Ω")
    print("  15 = √östeck√Ω")
    print("  16 = Libereck√Ω")
    print("  17 = Kr√°lov√©hradeck√Ω")
    print("  18 = Pardubick√Ω")
    print("  19 = Vysoƒçina")
    print("  20 = Jihomoravsk√Ω")
    print("  21 = Olomouck√Ω")
    print("  22 = Zl√≠nsk√Ω")
    print("  23 = Moravskoslezsk√Ω")
    locality_input = input("\nVyber kraje (10-23, oddƒõlen√© ƒç√°rkami) nebo Enter pro celou ƒåR: ").strip()

    locality_list = None
    if locality_input:
        localities = [l.strip() for l in locality_input.split(",")]
        locality_list = [int(l) for l in localities if l.isdigit()]

    # Str√°nky
    print("\nPoƒçet str√°nek:")
    pages_input = input("Max str√°nek (nebo 'all' pro v≈°echny) [5]: ").strip() or "5"
    if pages_input.lower() == "all":
        max_pages = None
        full_scan = True
    else:
        max_pages = int(pages_input) if pages_input.isdigit() else 5
        full_scan = False

    print("\n" + "="*80)
    print(f"‚úÖ Vybran√© parametry:")
    print(f"   Typy nemovitost√≠: {category_main_list}")
    print(f"   Typy inzer√°t≈Ø: {category_type_list}")
    print(f"   Kraje: {locality_list or 'Cel√° ƒåR'}")
    print(f"   Str√°nek: {'V≈†ECHNY' if full_scan else max_pages}")
    print("="*80)
    print()

    return {
        "category_main_list": category_main_list,
        "category_type_list": category_type_list,
        "locality_list": locality_list,
        "max_pages": max_pages,
        "full_scan": full_scan,
    }


def merge_records(all_records):
    """Slouƒç√≠ z√°znamy z v√≠ce scrapov√°n√≠."""
    # Pro fast scraper jen spoj√≠me v≈°echny z√°znamy
    # (ka≈æd√Ω m√° unik√°tn√≠ company + makl√©≈ô kombinaci)
    return all_records


def main():
    parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter)

    parser.add_argument("--prompt", action="store_true", help="Interaktivn√≠ v√Ωbƒõr parametr≈Ø")
    parser.add_argument("--category-main", type=int, default=1, help="1=Byty, 2=Domy, ...")
    parser.add_argument("--category-type", type=int, default=1, help="1=Prodej, 2=Pron√°jem, ...")
    parser.add_argument("--locality", type=int, help="10=Praha, 11=St≈ôedoƒçesk√Ω, ...")
    parser.add_argument("--max-pages", type=int, default=5, help="Max str√°nek [5]")
    parser.add_argument("--full-scan", action="store_true", help="V≈°echny str√°nky")
    parser.add_argument("-o", "--output", help="V√Ωstupn√≠ soubor")

    args = parser.parse_args()

    print("="*80)
    print("üöÄ SUPER RYCHL√ù SCRAPER MAKL√â≈ò≈Æ (s company API)")
    print("="*80)
    print()

    category_names = {1: "Byty", 2: "Domy", 3: "Pozemky", 4: "Komerƒçn√≠", 5: "Ostatn√≠"}
    type_names = {1: "Prodej", 2: "Pron√°jem", 3: "Dra≈æby"}
    region_names = {
        10: "Praha", 11: "St≈ôedoƒçesk√Ω", 12: "Jihoƒçesk√Ω", 13: "Plze≈àsk√Ω",
        14: "Karlovarsk√Ω", 15: "√östeck√Ω", 16: "Libereck√Ω", 17: "Kr√°lov√©hradeck√Ω",
        18: "Pardubick√Ω", 19: "Vysoƒçina", 20: "Jihomoravsk√Ω", 21: "Olomouck√Ω",
        22: "Zl√≠nsk√Ω", 23: "Moravskoslezsk√Ω"
    }

    try:
        scraper = SrealityScraper()

        if args.prompt:
            # Interaktivn√≠ m√≥d
            params = prompt_for_params()

            # Vytvo≈ô V≈†ECHNY kombinace najednou (pro deduplikaci!)
            combinations = []
            for category_main in params["category_main_list"]:
                for category_type in params["category_type_list"]:
                    localities = params["locality_list"] or [None]
                    for locality in localities:
                        combinations.append((category_main, category_type, locality))

            # V√Ωpis kombinac√≠
            print("\n" + "="*80)
            print(f"üéØ Celkem {len(combinations)} kombinac√≠ k zpracov√°n√≠:")
            for idx, (cat, typ, loc) in enumerate(combinations, 1):
                line = f"   {idx}. {category_names.get(cat)} / {type_names.get(typ)}"
                if loc:
                    line += f" / {region_names.get(loc)}"
                print(line)
            print("="*80)

            # Pou≈æij COMBINED funkci - automaticky deduplikuje companies!
            final_records = scrape_agents_fast_combined(
                scraper,
                combinations,
                params["max_pages"],
                params["full_scan"],
            )

        else:
            # Manu√°ln√≠ parametry
            print("üìã Parametry:")
            print(f"   ‚Ä¢ Typ: {category_names.get(args.category_main, 'Nezn√°m√Ω')}")
            print(f"   ‚Ä¢ Inzer√°t: {type_names.get(args.category_type, 'Nezn√°m√Ω')}")
            print(f"   ‚Ä¢ Kraj: {region_names.get(args.locality, 'Cel√° ƒåR')}")
            print(f"   ‚Ä¢ Str√°nek: {'V≈†ECHNY' if args.full_scan else args.max_pages}")
            print()

            final_records = scrape_agents_fast(
                scraper,
                args.category_main,
                args.category_type,
                args.locality,
                args.max_pages,
                args.full_scan,
            )

        if final_records:
            output = args.output or f"data/makleri_fast_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
            save_to_excel_hierarchical(final_records, output)
        else:
            print("‚ö†Ô∏è  ≈Ω√°dn√° data")

    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  P≈ôeru≈°eno")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Chyba: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

    print("\n" + "="*80)
    print("‚úÖ Hotovo!")
    print("="*80)


if __name__ == "__main__":
    main()
